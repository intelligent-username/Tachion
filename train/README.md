# Train

This folder contains the training scripts & configs. Preconditions are assumed!

We'll train an **XGBoost** model for predicting interest rate hikes/cuts, and a **DeepAR** models for everything else.

Note that the predictions for commodities won't be as strong, as they're affected by factors that aren't directly numeric, as discussed in the `data` folder [README](../data/README.md).

## File Structure

This folder contains the following important files:

```bash
├── loader.py         # Data loader (GluonTS compatible)
├── deep.py           # DeepAR model definition
├── tft.py            # TFT model definition
├── xg.py             # XGBoost classifier
├── train_deep.py     # Script for training DeepAR/TFT models
└── train_xgboost.py  # Script for training XGBoost interest rate classifier
```

## Usage

### Training XGBoost (Interest Rates)

```bash
python -m train.train_xgboost
```

Loads `data/interest/processed/Interest_Features.parquet` and trains a classifier.

### Training DeepAR / TFT (Time Series)

```bash
# Train TFT on crypto
python -m train.train_deep forex tft

# Train DeepAR on equities
python -m train.train_deep crypto deepar

# Options: crypto, equities, forex, comm
```

You can add a `-n` or `-y` at the end of each command to speed up the configuration prompting.

Model saved to `models/deepar_{asset}.pt`.

## Miscellaneous Notes

### DeepAR

DeepAR uses "a methodology for producing accurate probabilistic forecasts, based on training an auto-regressive LSTM-based recurrent network model on a large number of related time series" (Salinas et al., 2017). DeepAR uses negative-log likelihood for predictions and makes predictions in the form of Monte Carlo samples for computing quantile estaimtes. It also doesn't require much feature engineering, as the RNN is able to learn temporal patterns on its own. The RNN is also able to learn seasonal trends, which is especially helpful for something like crytpocurrencies.

Once the DeepAR model is trained, when making inferences, we don't actually need as long of a history window as we do for training. For example, we could just go with the past 10 days instead of the past 5 years. This'll speed up inferences significantly.

### XGBoost

We use Focal Loss for handling class imbalance, since hikes/cuts are much rarer than holds. SMOTE oversampling is also applied during training.

## Outputs

- Training scripts:
  - Import features generated by `data/*/formatter.py`
  - Align targets for the chosen prediction horizon
  - Train models and save artifacts in `models/`

All scripts are designed to be reusable across assets and quantiles.

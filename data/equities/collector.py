"""
Stock prices and S&P500 data collector
Calls the TwelveData API to collect information for the past ~15000 lines (about 5 years of TRADING DAYS)
(or less if not available) for the 79 stocks selected stocks.
It then collects the past ~15000 lines of S&P500 index data.
All data is written into CSVs in the raw/ directory.
"""

import json
import os
import datetime
import time

from core import call_specific_td

def write_data_eq(symbols):
    """
    Get ~15000 lines (3 API calls) worth of data for the given list of symbols and S&P500 index
    Using 30 minute intervals.
    The caller will ensure everything is written in chronological order.
    Writes to data/equities/raw/
    """
    path = os.path.join("data", "equities", "raw")
    os.makedirs(path, exist_ok=True)

    # This is for equities specifically, need ~15k
    num_calls = 3

    call_specific_td(path, symbols=["SPY"], num_calls = num_calls)
    call_specific_td(path, symbols=symbols, num_calls = num_calls)

    # note that the JSON records are written chronologically from newest to oldest
    # In the feature engineering (CSVs), remember to read backwards

if __name__ == "__main__":
    # 5 companies from each of the 11 S&P sectors
    # 9 more "important" large S&P companies for broader market conditions
    # And 15 "smaller" companies for variance injection
    # And of course the S&P data

    # For a total of 80

    print("Collecting data...")

    with open("data/equities/companies.txt", "r") as f:
        companies = [line.rstrip("\n") for line in f if line[0] != "#" and line != "\n"]
    
    write_data_eq(companies)

    print("Finished collecting data.")
    